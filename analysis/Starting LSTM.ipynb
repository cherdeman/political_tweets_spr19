{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import ast\n",
    "\n",
    "torch.manual_seed(1)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from utils.db_client import DBClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm trying\n",
      "Connected to political tweets DB\n"
     ]
    }
   ],
   "source": [
    "db = DBClient(secrets_path='../configs/db_secrets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = db.read(\"select tweet_text_clean, 1 from staging.master limit 10\")\n",
    "#example = examples[0]\n",
    "examples = [(ast.literal_eval(example[0]), example[1]) for example in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['side',\n",
       "   'need',\n",
       "   'learn',\n",
       "   'firearm',\n",
       "   'current',\n",
       "   'process',\n",
       "   'weve',\n",
       "   'get',\n",
       "   'keep',\n",
       "   'firearm',\n",
       "   'secure',\n",
       "   'minor',\n",
       "   'people',\n",
       "   'restrain',\n",
       "   'order',\n",
       "   'shouldnt',\n",
       "   'weapon',\n",
       "   'need',\n",
       "   'improve',\n",
       "   'current',\n",
       "   'background',\n",
       "   'check',\n",
       "   'system'],\n",
       "  1),\n",
       " (['meeting',\n",
       "   'north',\n",
       "   'korea',\n",
       "   'cancel',\n",
       "   'northrup',\n",
       "   'grummans',\n",
       "   'stock',\n",
       "   'soar',\n",
       "   'love',\n",
       "   'work',\n",
       "   'corporation',\n",
       "   '#american',\n",
       "   'people',\n",
       "   'receive',\n",
       "   '36000',\n",
       "   'ng',\n",
       "   'buy',\n",
       "   'let',\n",
       "   '#change',\n",
       "   '#congress',\n",
       "   'put',\n",
       "   '#people',\n",
       "   '#politics',\n",
       "   '#va08',\n",
       "   '#gop'],\n",
       "  1),\n",
       " (['west',\n",
       "   'coast',\n",
       "   'attempt',\n",
       "   'literally',\n",
       "   'limit',\n",
       "   'first',\n",
       "   'amendment',\n",
       "   'punish',\n",
       "   'business',\n",
       "   'create',\n",
       "   'job',\n",
       "   'idolized',\n",
       "   'worship',\n",
       "   'pornstar',\n",
       "   'reason',\n",
       "   'try',\n",
       "   'make',\n",
       "   'trump',\n",
       "   'look',\n",
       "   'bad',\n",
       "   'thank',\n",
       "   'god',\n",
       "   'im',\n",
       "   'safe',\n",
       "   'side',\n",
       "   'country'],\n",
       "  1),\n",
       " (['say',\n",
       "   'everyone',\n",
       "   'play',\n",
       "   'game',\n",
       "   'regard',\n",
       "   'talk',\n",
       "   'north',\n",
       "   'korea',\n",
       "   'cancel',\n",
       "   'summit',\n",
       "   'remind',\n",
       "   'bitter',\n",
       "   'ex',\n",
       "   'say',\n",
       "   'thing',\n",
       "   'cheater',\n",
       "   '#trump',\n",
       "   '#cheater'],\n",
       "  1),\n",
       " (['democrat',\n",
       "   'foreign',\n",
       "   'invader',\n",
       "   'godgiven',\n",
       "   'hardwon',\n",
       "   'civil',\n",
       "   'liberty',\n",
       "   '#happymemorialday',\n",
       "   'veterans',\n",
       "   'service',\n",
       "   'person',\n",
       "   'die',\n",
       "   'protect',\n",
       "   'american',\n",
       "   'progressive',\n",
       "   'communism',\n",
       "   'shut',\n",
       "   'left',\n",
       "   '#bluewave2018'],\n",
       "  1),\n",
       " (['help',\n",
       "   'im',\n",
       "   'try',\n",
       "   'find',\n",
       "   'plan',\n",
       "   'make',\n",
       "   'health',\n",
       "   'care',\n",
       "   'accessible',\n",
       "   'affordable',\n",
       "   'last',\n",
       "   'year',\n",
       "   'say',\n",
       "   'bridge',\n",
       "   'kind',\n",
       "   'see',\n",
       "   'american',\n",
       "   'acquire',\n",
       "   'health',\n",
       "   'coverage',\n",
       "   'aca',\n",
       "   'remain',\n",
       "   'covered',\n",
       "   'offer',\n",
       "   'good',\n",
       "   'option'],\n",
       "  1),\n",
       " (['bring',\n",
       "   'cost',\n",
       "   'health',\n",
       "   'care',\n",
       "   'cover',\n",
       "   'everyone',\n",
       "   'like',\n",
       "   'many',\n",
       "   'country',\n",
       "   'elect',\n",
       "   'representative',\n",
       "   'care',\n",
       "   'health',\n",
       "   'corporate',\n",
       "   'special',\n",
       "   'interest',\n",
       "   'trey',\n",
       "   'hollingsworth',\n",
       "   'never',\n",
       "   'ever'],\n",
       "  1),\n",
       " (['good', 'job', 'gop', 'harm', 'worker'], 1),\n",
       " (['small',\n",
       "   'bank',\n",
       "   'regulatory',\n",
       "   'relief',\n",
       "   'victory',\n",
       "   'main',\n",
       "   'st',\n",
       "   'small',\n",
       "   'business',\n",
       "   'rhode',\n",
       "   'island',\n",
       "   'community',\n",
       "   'read'],\n",
       "  1),\n",
       " (['listen',\n",
       "   'interview',\n",
       "   'last',\n",
       "   'night',\n",
       "   'wonderful',\n",
       "   'discuss',\n",
       "   'importance',\n",
       "   'strong',\n",
       "   'national',\n",
       "   'security',\n",
       "   'lukewarm',\n",
       "   'relationship',\n",
       "   'friend',\n",
       "   'appeasement',\n",
       "   'enemy',\n",
       "   'congressman',\n",
       "   'believe'],\n",
       "  1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'side': 0, 'need': 1, 'learn': 2, 'firearm': 3, 'current': 4, 'process': 5, 'weve': 6, 'get': 7, 'keep': 8, 'secure': 9, 'minor': 10, 'people': 11, 'restrain': 12, 'order': 13, 'shouldnt': 14, 'weapon': 15, 'improve': 16, 'background': 17, 'check': 18, 'system': 19, 'meeting': 20, 'north': 21, 'korea': 22, 'cancel': 23, 'northrup': 24, 'grummans': 25, 'stock': 26, 'soar': 27, 'love': 28, 'work': 29, 'corporation': 30, '#american': 31, 'receive': 32, '36000': 33, 'ng': 34, 'buy': 35, 'let': 36, '#change': 37, '#congress': 38, 'put': 39, '#people': 40, '#politics': 41, '#va08': 42, '#gop': 43, 'west': 44, 'coast': 45, 'attempt': 46, 'literally': 47, 'limit': 48, 'first': 49, 'amendment': 50, 'punish': 51, 'business': 52, 'create': 53, 'job': 54, 'idolized': 55, 'worship': 56, 'pornstar': 57, 'reason': 58, 'try': 59, 'make': 60, 'trump': 61, 'look': 62, 'bad': 63, 'thank': 64, 'god': 65, 'im': 66, 'safe': 67, 'country': 68, 'say': 69, 'everyone': 70, 'play': 71, 'game': 72, 'regard': 73, 'talk': 74, 'summit': 75, 'remind': 76, 'bitter': 77, 'ex': 78, 'thing': 79, 'cheater': 80, '#trump': 81, '#cheater': 82, 'democrat': 83, 'foreign': 84, 'invader': 85, 'godgiven': 86, 'hardwon': 87, 'civil': 88, 'liberty': 89, '#happymemorialday': 90, 'veterans': 91, 'service': 92, 'person': 93, 'die': 94, 'protect': 95, 'american': 96, 'progressive': 97, 'communism': 98, 'shut': 99, 'left': 100, '#bluewave2018': 101, 'help': 102, 'find': 103, 'plan': 104, 'health': 105, 'care': 106, 'accessible': 107, 'affordable': 108, 'last': 109, 'year': 110, 'bridge': 111, 'kind': 112, 'see': 113, 'acquire': 114, 'coverage': 115, 'aca': 116, 'remain': 117, 'covered': 118, 'offer': 119, 'good': 120, 'option': 121, 'bring': 122, 'cost': 123, 'cover': 124, 'like': 125, 'many': 126, 'elect': 127, 'representative': 128, 'corporate': 129, 'special': 130, 'interest': 131, 'trey': 132, 'hollingsworth': 133, 'never': 134, 'ever': 135, 'gop': 136, 'harm': 137, 'worker': 138, 'small': 139, 'bank': 140, 'regulatory': 141, 'relief': 142, 'victory': 143, 'main': 144, 'st': 145, 'rhode': 146, 'island': 147, 'community': 148, 'read': 149, 'listen': 150, 'interview': 151, 'night': 152, 'wonderful': 153, 'discuss': 154, 'importance': 155, 'strong': 156, 'national': 157, 'security': 158, 'lukewarm': 159, 'relationship': 160, 'friend': 161, 'appeasement': 162, 'enemy': 163, 'congressman': 164, 'believe': 165}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "word_to_ix = {}\n",
    "for tweet, tag in examples:\n",
    "    for word in tweet:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {0: 0,  4: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.weights = torch.FloatTensor(model.wv.vectors)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(self.weights)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3146, -0.3128],\n",
      "        [-1.3141, -0.3130],\n",
      "        [-1.3186, -0.3113],\n",
      "        [-1.2697, -0.3298],\n",
      "        [-1.3589, -0.2970],\n",
      "        [-1.3421, -0.3029],\n",
      "        [-1.3518, -0.2995],\n",
      "        [-1.3694, -0.2934],\n",
      "        [-1.3176, -0.3117],\n",
      "        [-1.2744, -0.3279],\n",
      "        [-1.3539, -0.2987],\n",
      "        [-1.4093, -0.2801],\n",
      "        [-1.4203, -0.2766],\n",
      "        [-1.3962, -0.2844],\n",
      "        [-1.3570, -0.2976],\n",
      "        [-1.3935, -0.2853],\n",
      "        [-1.3532, -0.2990],\n",
      "        [-1.4056, -0.2813],\n",
      "        [-1.3572, -0.2976],\n",
      "        [-1.4032, -0.2821],\n",
      "        [-1.2545, -0.3358],\n",
      "        [-1.2358, -0.3433],\n",
      "        [-1.3295, -0.3073]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-6f64ba7de65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Tensors of word indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msentence_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Step 3. Run our forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-6e536f25949c>\u001b[0m in \u001b[0;36mprepare_sequence\u001b[0;34m(seq, to_ix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_to_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMSentiment(300, 3, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.1)\n",
    "\n",
    "training_data = examples\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = lstm_model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(2):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        lstm_model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = lstm_model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMSentiment(\n",
       "  (word_embeddings): Embedding(3000000, 300)\n",
       "  (lstm): LSTM(3, 3)\n",
       "  (hidden2tag): Linear(in_features=3, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
