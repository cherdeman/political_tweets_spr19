{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import gensim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "\n",
    "from utils.db_client import DBClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm trying\n",
      "Connected to political tweets DB\n"
     ]
    }
   ],
   "source": [
    "# Make DB Connection\n",
    "db = DBClient(secrets_path='../configs/db_secrets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "examples = db.read(\"select tweet_text_clean, label from staging.train_twitter140 order by Random() limit 100000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fields\n",
    "txt_field = data.Field(sequential=True, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True)\n",
    "label_field = data.Field(sequential=False, \n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None)\n",
    "train_val_fields = [\n",
    "    ('SentimentText', txt_field), # process it as text\n",
    "    ('Sentiment', label_field) # process it as label\n",
    "]\n",
    "\n",
    "# Convert text ecamples to Example datatype\n",
    "examples = [data.Example.fromlist(((ast.literal_eval(example[0])), example[1]), train_val_fields) for example in examples]\n",
    "\n",
    "# Create dataset\n",
    "dataset = data.Dataset(examples, train_val_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "# Make vocab for text and label fields\n",
    "txt_field.build_vocab(dataset,\n",
    "                      max_size = MAX_VOCAB_SIZE, \n",
    "                      vectors = vocab.Vectors('glove.twitter.27B.50d.txt', '../model/glove.twitter.27B/'))\n",
    "label_field.build_vocab(dataset)\n",
    "\n",
    "pretrained_embeddings = txt_field.vocab.vectors\n",
    "\n",
    "# Make train/val/test splits\n",
    "train_data, test_data, valid_data = dataset.split([0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 70000\n",
      "Number of validation examples: 20000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                          batch_sizes = (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "                                                                        sort_key = lambda x: len(x.SentimentText),\n",
    "                                                                        sort_within_batch=True,\n",
    "                                                                        repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):#, optimizer, criterion):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           #bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.optimizer = None #optimizer #optim.Adam(model.parameters())\n",
    "        self.criterion = None #criterion #nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    \n",
    "    def set_pretrained_weights(self, pretrained_embeddings, txt_field):\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "        UNK_IDX = txt_field.vocab.stoi[txt_field.unk_token]\n",
    "\n",
    "        self.embedding.weight.data[UNK_IDX] = torch.zeros(self.embedding_dim)\n",
    "        self.embedding.weight.data[self.pad_idx] = torch.zeros(self.embedding_dim)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.train()\n",
    "        optimizer = self.optimizer\n",
    "        for batch in iterator:\n",
    "            if len(batch) == BATCH_SIZE:\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text, text_lengths = batch.SentimentText\n",
    "                predictions = self.forward(text, text_lengths).squeeze(1)\n",
    "                loss = self.criterion(predictions, batch.Sentiment.float())\n",
    "\n",
    "                acc = self.binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "\n",
    "                    text, text_lengths = batch.SentimentText\n",
    "\n",
    "                    predictions = self.forward(text, text_lengths).squeeze(1)\n",
    "\n",
    "                    loss = self.criterion(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    acc = self.binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def sentiment_political(self, iterator):\n",
    "\n",
    "        # keep?\n",
    "        model.eval()\n",
    "        \n",
    "        sentiment_df = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "\n",
    "                    text, text_lengths = batch.SentimentText\n",
    "\n",
    "                    predictions = self.forward(text, text_lengths).squeeze(1)\n",
    "                    \n",
    "                    preds = torch.sigmoid(predictions)\n",
    "                    \n",
    "                    pred_arr = preds.numpy()\n",
    "                    id_arr = batch.Id.numpy()\n",
    "                    pred_class_arr = torch.round(preds).numpy()\n",
    "                    \n",
    "                    if not sentiment_df:\n",
    "                        sentiment_df = pd.DataFrame({\"tweet_id\": id_arr, \n",
    "                                                     \"prediction_raw\": pred_arr, \n",
    "                                                     \"prediction_class\": pred_clas_arr})\n",
    "                    else:\n",
    "                        sentiment_df = pd.concat([sentiment_df, pd.DataFrame({\"tweet_id\": id_arr, \n",
    "                                                     \"prediction_raw\": pred_arr, \n",
    "                                                     \"prediction_class\": pred_clas_arr})])\n",
    "\n",
    "                    #loss = self.criterion(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    #acc = self.binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    #epoch_loss += loss.item()\n",
    "                    #epoch_acc += acc.item()\n",
    "        return sentiment_df\n",
    "    \n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_training(N_EPOCHS, model, train_iterator, valid_iterator):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = model.train_epoch(train_iterator)\n",
    "        valid_loss, valid_acc = model.evaluate(valid_iterator)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = model.epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\"Adam\", \"Adagrad\"]\n",
    "criterions = [\"BCEWithLogitsLoss\"] \n",
    "learning_rates = [0.001, 0.01]\n",
    "epochs = [5, 25] \n",
    "weight_decay = [0, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on model Adam-0.001-0-BCEWithLogitsLoss-5\n",
      "Epoch: 01 | Epoch Time: 1m 52s\n",
      "\tTrain Loss: 0.563 | Train Acc: 70.48%\n",
      "\t Val. Loss: 0.493 |  Val. Acc: 75.84%\n",
      "Epoch: 02 | Epoch Time: 2m 9s\n",
      "\tTrain Loss: 0.508 | Train Acc: 74.94%\n",
      "\t Val. Loss: 0.480 |  Val. Acc: 76.72%\n",
      "Epoch: 03 | Epoch Time: 2m 14s\n",
      "\tTrain Loss: 0.485 | Train Acc: 76.46%\n",
      "\t Val. Loss: 0.467 |  Val. Acc: 77.03%\n",
      "Epoch: 04 | Epoch Time: 2m 16s\n",
      "\tTrain Loss: 0.467 | Train Acc: 77.82%\n",
      "\t Val. Loss: 0.466 |  Val. Acc: 77.24%\n",
      "Epoch: 05 | Epoch Time: 2m 20s\n",
      "\tTrain Loss: 0.449 | Train Acc: 78.89%\n",
      "\t Val. Loss: 0.473 |  Val. Acc: 77.31%\n",
      "working on model Adam-0.001-0-BCEWithLogitsLoss-50\n",
      "Epoch: 01 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.436 | Train Acc: 79.75%\n",
      "\t Val. Loss: 0.468 |  Val. Acc: 77.57%\n",
      "Epoch: 02 | Epoch Time: 2m 20s\n",
      "\tTrain Loss: 0.422 | Train Acc: 80.43%\n",
      "\t Val. Loss: 0.475 |  Val. Acc: 77.49%\n",
      "Epoch: 03 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.410 | Train Acc: 81.16%\n",
      "\t Val. Loss: 0.475 |  Val. Acc: 77.52%\n",
      "Epoch: 04 | Epoch Time: 2m 19s\n",
      "\tTrain Loss: 0.397 | Train Acc: 82.02%\n",
      "\t Val. Loss: 0.481 |  Val. Acc: 77.40%\n",
      "Epoch: 05 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.388 | Train Acc: 82.43%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 76.97%\n",
      "Epoch: 06 | Epoch Time: 2m 20s\n",
      "\tTrain Loss: 0.373 | Train Acc: 83.18%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 76.79%\n",
      "Epoch: 07 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.368 | Train Acc: 83.50%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 76.95%\n",
      "Epoch: 08 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.357 | Train Acc: 83.99%\n",
      "\t Val. Loss: 0.510 |  Val. Acc: 76.68%\n",
      "Epoch: 09 | Epoch Time: 2m 19s\n",
      "\tTrain Loss: 0.348 | Train Acc: 84.52%\n",
      "\t Val. Loss: 0.532 |  Val. Acc: 76.47%\n",
      "Epoch: 10 | Epoch Time: 2m 18s\n",
      "\tTrain Loss: 0.340 | Train Acc: 84.80%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 76.37%\n",
      "Epoch: 11 | Epoch Time: 2m 17s\n",
      "\tTrain Loss: 0.328 | Train Acc: 85.22%\n",
      "\t Val. Loss: 0.572 |  Val. Acc: 76.40%\n",
      "Epoch: 12 | Epoch Time: 2m 17s\n",
      "\tTrain Loss: 0.321 | Train Acc: 85.76%\n",
      "\t Val. Loss: 0.569 |  Val. Acc: 76.42%\n",
      "Epoch: 13 | Epoch Time: 2m 17s\n",
      "\tTrain Loss: 0.317 | Train Acc: 85.88%\n",
      "\t Val. Loss: 0.566 |  Val. Acc: 76.34%\n",
      "Epoch: 14 | Epoch Time: 2m 17s\n",
      "\tTrain Loss: 0.306 | Train Acc: 86.46%\n",
      "\t Val. Loss: 0.603 |  Val. Acc: 75.89%\n",
      "Epoch: 15 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.301 | Train Acc: 86.64%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 75.80%\n",
      "Epoch: 16 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.293 | Train Acc: 86.91%\n",
      "\t Val. Loss: 0.641 |  Val. Acc: 75.73%\n",
      "Epoch: 17 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.288 | Train Acc: 87.23%\n",
      "\t Val. Loss: 0.629 |  Val. Acc: 75.40%\n",
      "Epoch: 18 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.281 | Train Acc: 87.49%\n",
      "\t Val. Loss: 0.654 |  Val. Acc: 75.31%\n",
      "Epoch: 19 | Epoch Time: 2m 20s\n",
      "\tTrain Loss: 0.273 | Train Acc: 88.02%\n",
      "\t Val. Loss: 0.657 |  Val. Acc: 75.39%\n",
      "Epoch: 20 | Epoch Time: 2m 19s\n",
      "\tTrain Loss: 0.269 | Train Acc: 88.19%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 75.37%\n",
      "Epoch: 21 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.262 | Train Acc: 88.40%\n",
      "\t Val. Loss: 0.723 |  Val. Acc: 75.23%\n",
      "Epoch: 22 | Epoch Time: 2m 18s\n",
      "\tTrain Loss: 0.258 | Train Acc: 88.79%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 75.12%\n",
      "Epoch: 23 | Epoch Time: 2m 48s\n",
      "\tTrain Loss: 0.252 | Train Acc: 88.96%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 75.10%\n",
      "Epoch: 24 | Epoch Time: 2m 18s\n",
      "\tTrain Loss: 0.248 | Train Acc: 89.03%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 74.88%\n",
      "Epoch: 25 | Epoch Time: 2m 18s\n",
      "\tTrain Loss: 0.245 | Train Acc: 89.20%\n",
      "\t Val. Loss: 0.714 |  Val. Acc: 75.09%\n",
      "Epoch: 26 | Epoch Time: 2m 20s\n",
      "\tTrain Loss: 0.237 | Train Acc: 89.66%\n",
      "\t Val. Loss: 0.779 |  Val. Acc: 74.65%\n",
      "Epoch: 27 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.234 | Train Acc: 89.72%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 74.70%\n",
      "Epoch: 28 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.228 | Train Acc: 90.08%\n",
      "\t Val. Loss: 0.762 |  Val. Acc: 75.05%\n",
      "Epoch: 29 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.225 | Train Acc: 90.13%\n",
      "\t Val. Loss: 0.787 |  Val. Acc: 74.91%\n",
      "Epoch: 30 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.220 | Train Acc: 90.48%\n",
      "\t Val. Loss: 0.806 |  Val. Acc: 74.83%\n",
      "Epoch: 31 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.215 | Train Acc: 90.77%\n",
      "\t Val. Loss: 0.838 |  Val. Acc: 74.45%\n",
      "Epoch: 32 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.214 | Train Acc: 90.75%\n",
      "\t Val. Loss: 0.822 |  Val. Acc: 74.72%\n",
      "Epoch: 33 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.212 | Train Acc: 90.90%\n",
      "\t Val. Loss: 0.851 |  Val. Acc: 74.64%\n",
      "Epoch: 34 | Epoch Time: 2m 26s\n",
      "\tTrain Loss: 0.209 | Train Acc: 90.87%\n",
      "\t Val. Loss: 0.853 |  Val. Acc: 74.69%\n",
      "Epoch: 35 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.204 | Train Acc: 91.15%\n",
      "\t Val. Loss: 0.859 |  Val. Acc: 74.74%\n",
      "Epoch: 36 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.202 | Train Acc: 91.40%\n",
      "\t Val. Loss: 0.887 |  Val. Acc: 74.50%\n",
      "Epoch: 37 | Epoch Time: 2m 25s\n",
      "\tTrain Loss: 0.200 | Train Acc: 91.43%\n",
      "\t Val. Loss: 0.910 |  Val. Acc: 74.72%\n",
      "Epoch: 38 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.197 | Train Acc: 91.53%\n",
      "\t Val. Loss: 0.836 |  Val. Acc: 74.50%\n",
      "Epoch: 39 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.192 | Train Acc: 91.74%\n",
      "\t Val. Loss: 0.897 |  Val. Acc: 74.33%\n",
      "Epoch: 40 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.190 | Train Acc: 91.91%\n",
      "\t Val. Loss: 0.896 |  Val. Acc: 74.31%\n",
      "Epoch: 41 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.187 | Train Acc: 91.92%\n",
      "\t Val. Loss: 0.922 |  Val. Acc: 74.35%\n",
      "Epoch: 42 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.184 | Train Acc: 92.05%\n",
      "\t Val. Loss: 0.901 |  Val. Acc: 74.05%\n",
      "Epoch: 43 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.181 | Train Acc: 92.17%\n",
      "\t Val. Loss: 0.934 |  Val. Acc: 73.95%\n",
      "Epoch: 44 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.177 | Train Acc: 92.34%\n",
      "\t Val. Loss: 0.989 |  Val. Acc: 74.04%\n",
      "Epoch: 45 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.180 | Train Acc: 92.34%\n",
      "\t Val. Loss: 0.936 |  Val. Acc: 74.04%\n",
      "Epoch: 46 | Epoch Time: 2m 21s\n",
      "\tTrain Loss: 0.176 | Train Acc: 92.46%\n",
      "\t Val. Loss: 0.911 |  Val. Acc: 74.01%\n",
      "Epoch: 47 | Epoch Time: 2m 22s\n",
      "\tTrain Loss: 0.175 | Train Acc: 92.54%\n",
      "\t Val. Loss: 0.932 |  Val. Acc: 74.03%\n",
      "Epoch: 48 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.171 | Train Acc: 92.66%\n",
      "\t Val. Loss: 0.969 |  Val. Acc: 73.73%\n",
      "Epoch: 49 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.171 | Train Acc: 92.84%\n",
      "\t Val. Loss: 0.970 |  Val. Acc: 74.26%\n",
      "Epoch: 50 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.167 | Train Acc: 92.84%\n",
      "\t Val. Loss: 0.983 |  Val. Acc: 73.98%\n",
      "working on model Adam-0.001-0-BCEWithLogitsLoss-100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fb45ea53b24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"working on model {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ff15ba4efac8>\u001b[0m in \u001b[0;36mepoch_training\u001b[0;34m(N_EPOCHS, model, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1794021747d6>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aml/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(txt_field.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = txt_field.vocab.stoi[txt_field.pad_token]\n",
    "\n",
    "\n",
    "best_model_tuple = None\n",
    "best_accuracy = -1\n",
    "for optimizer in optimizers:\n",
    "    # Make a new model at the start\n",
    "    model = LSTM(INPUT_DIM, \n",
    "                EMBEDDING_DIM, \n",
    "                HIDDEN_DIM, \n",
    "                OUTPUT_DIM, \n",
    "                N_LAYERS,\n",
    "                DROPOUT, \n",
    "                PAD_IDX)\n",
    "    model.set_pretrained_weights(pretrained_embeddings, txt_field)\n",
    "    for learning_rate in learning_rates:\n",
    "        for decay in weight_decay:\n",
    "            # Set OPTIMIZER\n",
    "            if optimizer == \"Adam\":\n",
    "                model.optimizer = optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay = decay) \n",
    "            else: \n",
    "                model.optimizer = optim.Adagrad(params = model.parameters(), lr = learning_rate, weight_decay = decay)\n",
    "\n",
    "            for criterion in criterions:\n",
    "                # SET CRITERION\n",
    "                if criterion == \"BCEWithLogitsLoss\":\n",
    "                    model.criterion = nn.BCEWithLogitsLoss() \n",
    "                else: \n",
    "                    model.criterion = nn.NLLLoss()\n",
    "                \n",
    "                for epoch in epochs:\n",
    "                    model_name = \"-\".join([optimizer, str(learning_rate), str(decay), str(criterion), str(epoch)])\n",
    "\n",
    "                    print(f\"working on model {model_name}\")\n",
    "                    trained_model = epoch_training(epoch, model, train_iterator, valid_iterator)\n",
    "                    _, acc = trained_model.evaluate(valid_iterator)\n",
    "                    \n",
    "                    if acc > best_accuracy:\n",
    "                        best_accuracy = acc\n",
    "                        best_model_tuple = (model_name, trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = best_model_tuple[0]\n",
    "best_model = best_model_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam-0.001-0-BCEWithLogitsLoss-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0419494485019878, 0.7395501592356688)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_model_name)\n",
    "best_model.evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_no = 1\n",
    "torch.save(best_model.state_dict(), 'models_store/best_model_dict_{}_{}.pt'.format(best_model_name, run_no))\n",
    "torch.save(best_model, 'models_store/best_model_{}_{}.pt'.format(best_model_name, run_no))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "num_tweets = 100000\n",
    "topics = ['health', 'econ_jobs', 'guns']\n",
    "topics_str = ', '.join(topics)\n",
    "examples = db.read(\"select tweet_id, tweet_text_clean, {} from staging.master order by Random() limit {}\".format(topics_str, num_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fields\n",
    "txt_field = data.Field(sequential=True, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True)\n",
    "id_field = data.Field(sequential=False, \n",
    "                      use_vocab=False, \n",
    "                      pad_token=None, \n",
    "                      unk_token=None)\n",
    "\n",
    "eval_val_fields = [\n",
    "    ('SentimentText', txt_field), # process it as text\n",
    "    ('Id', id_field) # process it as id\n",
    "]\n",
    "\n",
    "\n",
    "# Convert text examples to Example datatype\n",
    "examples = [data.Example.fromlist(((ast.literal_eval(example[0])), example[1]), train_val_fields) for example in examples]\n",
    "\n",
    "# Create dataset\n",
    "dataset = data.Dataset(examples, eval_val_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iterator\n",
    "eval_iterator = data.BucketIterator(dataset, batch_size = BATCH_SIZE, sort_key = lambda x: len(x.SentimentText),\n",
    "                                                                        sort_within_batch=True,\n",
    "                                                                        repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'sentiment_political'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-789fb17458c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate sentiment of each tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiment_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_political\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentiment_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models_store/political_sentiment_{}_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'sentiment_political'"
     ]
    }
   ],
   "source": [
    "# Calculate sentiment of each tweet\n",
    "sentiment_df = best_model.sentiment_political(eval_iterator)\n",
    "sentiment_df.to_csv('models_store/political_sentiment_{}_{}.csv'.format(best_model_name, run_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = ['dem-lead', 'dem-base', 'rep-lead', 'rep-base']\n",
    "palette = {\"dem-base\": \"#a8b2ff\",\"dem-lead\": \"#0015bc\", \"rep-base\": \"#ff9d9d\", \"rep-lead\": \"#ff0000\", \"\": \"gray\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def binary_accuracy(preds, y):\n",
    "#     \"\"\"\n",
    "#     Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "#     \"\"\"\n",
    "\n",
    "#     #round predictions to the closest integer\n",
    "#     rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "#     correct = (rounded_preds == y).float() #convert into float for division \n",
    "#     acc = correct.sum() / len(correct)\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train(model, iterator):\n",
    "    \n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "    \n",
    "#     model.train()\n",
    "    \n",
    "#     for batch in iterator:\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         text, text_lengths = batch.SentimentText\n",
    "        \n",
    "#         predictions = model(text, text_lengths).squeeze(1)\n",
    "\n",
    "#         loss = model.criterion(predictions, batch.Sentiment.float())\n",
    "        \n",
    "#         acc = binary_accuracy(predictions, batch.Sentiment.float())\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         model.optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "#         epoch_acc += acc.item()\n",
    "        \n",
    "#     return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def evaluate(model, iterator):\n",
    "    \n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "    \n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "    \n",
    "#         for batch in iterator:\n",
    "            \n",
    "#             text, text_lengths = batch.SentimentText\n",
    "\n",
    "#             predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "#             loss = model.criterion(predictions, batch.Sentiment.float())\n",
    "            \n",
    "#             acc = binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "#             epoch_acc += acc.item()\n",
    "        \n",
    "#     return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def epoch_time(start_time, end_time):\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     elapsed_mins = int(elapsed_time / 60)\n",
    "#     elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "#     return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(txt_field.vocab)\n",
    "# EMBEDDING_DIM = 50\n",
    "# HIDDEN_DIM = 256\n",
    "# OUTPUT_DIM = 1\n",
    "# N_LAYERS = 2\n",
    "# DROPOUT = 0.5\n",
    "# PAD_IDX = txt_field.vocab.stoi[txt_field.pad_token]\n",
    "# OPTIMIZER = optim.Adam(model.parameters())\n",
    "# CRITERION = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# model = LSTM(INPUT_DIM, \n",
    "#             EMBEDDING_DIM, \n",
    "#             HIDDEN_DIM, \n",
    "#             OUTPUT_DIM, \n",
    "#             N_LAYERS,\n",
    "#             DROPOUT, \n",
    "#             PAD_IDX,\n",
    "#             OPTIMIZER,\n",
    "#             CRITERION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = epoch_training(10, model, train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZER = optim.Adam(model.parameters())\n",
    "#CRITERION = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# model = LSTM(INPUT_DIM, \n",
    "#             EMBEDDING_DIM, \n",
    "#             HIDDEN_DIM, \n",
    "#             OUTPUT_DIM, \n",
    "#             N_LAYERS,\n",
    "#             DROPOUT, \n",
    "#             PAD_IDX,\n",
    "#             OPTIMIZER,\n",
    "#             CRITERION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bea4f7f6c263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The model has {count_parameters(model):,} trainable parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
