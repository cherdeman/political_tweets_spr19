{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import gensim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.db_client import DBClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm trying\n",
      "Connected to political tweets DB\n"
     ]
    }
   ],
   "source": [
    "# Make DB Connection\n",
    "db = DBClient(secrets_path='../configs/db_secrets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "examples = db.read(\"select tweet_text_clean, label from staging.train_twitter140 order by Random() limit 100000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fields\n",
    "txt_field = data.Field(sequential=True, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True)\n",
    "label_field = data.Field(sequential=False, \n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None)\n",
    "train_val_fields = [\n",
    "    ('SentimentText', txt_field), # process it as text\n",
    "    ('Sentiment', label_field) # process it as label\n",
    "]\n",
    "\n",
    "# Convert text ecamples to Example datatype\n",
    "examples = [data.Example.fromlist(((ast.literal_eval(example[0])), example[1]), train_val_fields) for example in examples]\n",
    "\n",
    "# Create dataset\n",
    "dataset = data.Dataset(examples, train_val_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " 'im',\n",
       " 'get',\n",
       " 'go',\n",
       " 'day',\n",
       " 'good',\n",
       " 'work',\n",
       " 'like',\n",
       " 'love',\n",
       " 'dont',\n",
       " 'today',\n",
       " 'time',\n",
       " 'cant',\n",
       " 'think',\n",
       " 'know',\n",
       " 'back',\n",
       " 'want',\n",
       " 'make',\n",
       " 'see',\n",
       " 'well',\n",
       " 'miss',\n",
       " 'one',\n",
       " 'lol',\n",
       " 'really',\n",
       " 'feel',\n",
       " 'need',\n",
       " 'come',\n",
       " 'night',\n",
       " 'still',\n",
       " 'new',\n",
       " 'watch',\n",
       " 'look',\n",
       " 'home',\n",
       " 'thanks',\n",
       " 'oh',\n",
       " 'much',\n",
       " 'last',\n",
       " 'morning',\n",
       " 'hope',\n",
       " 'say',\n",
       " 'great',\n",
       " 'twitter',\n",
       " 'sleep',\n",
       " 'tomorrow',\n",
       " 'take',\n",
       " 'wait',\n",
       " 'wish',\n",
       " 'ill',\n",
       " 'bad',\n",
       " 'thats',\n",
       " 'haha',\n",
       " 'sad',\n",
       " 'try',\n",
       " 'fun',\n",
       " 'right',\n",
       " 'didnt',\n",
       " 'week',\n",
       " 'thing',\n",
       " 'happy',\n",
       " 'would',\n",
       " 'sorry',\n",
       " 'way',\n",
       " 'friend',\n",
       " 'tonight',\n",
       " 'find',\n",
       " 'gonna',\n",
       " 'nice',\n",
       " 'leave',\n",
       " 'though',\n",
       " 'ive',\n",
       " 'bed',\n",
       " 'could',\n",
       " 'start',\n",
       " 'tweet',\n",
       " 'youre',\n",
       " 'people',\n",
       " 'hour',\n",
       " 'show',\n",
       " 'hate',\n",
       " 'yeah',\n",
       " 'school',\n",
       " 'guy',\n",
       " 'weekend',\n",
       " 'yes',\n",
       " 'play',\n",
       " 'hey',\n",
       " 'even',\n",
       " 'never',\n",
       " 'let',\n",
       " 'next',\n",
       " 'thank',\n",
       " 'follow',\n",
       " 'awesome',\n",
       " 'long',\n",
       " 'use',\n",
       " 'soon',\n",
       " 'little',\n",
       " 'best',\n",
       " 'eat',\n",
       " 'everyone',\n",
       " 'tell',\n",
       " 'wanna',\n",
       " 'keep',\n",
       " 'movie',\n",
       " 'ok',\n",
       " 'first',\n",
       " 'call',\n",
       " 'year',\n",
       " 'please',\n",
       " 'wont',\n",
       " 'life',\n",
       " 'sick',\n",
       " 'help',\n",
       " 'always',\n",
       " 'rain',\n",
       " 'sure',\n",
       " 'girl',\n",
       " 'suck',\n",
       " 'give',\n",
       " 'wake',\n",
       " 'already',\n",
       " 'another',\n",
       " 'cool',\n",
       " 'talk',\n",
       " 'head',\n",
       " 'lot',\n",
       " 'doesnt',\n",
       " 'hurt',\n",
       " 'phone',\n",
       " 'ready',\n",
       " 'something',\n",
       " 'sound',\n",
       " 'lose',\n",
       " 'hear',\n",
       " 'yet',\n",
       " 'early',\n",
       " 'old',\n",
       " 'song',\n",
       " 'mean',\n",
       " 'man',\n",
       " 'house',\n",
       " 'ur',\n",
       " 'enjoy',\n",
       " 'big',\n",
       " 'ever',\n",
       " 'pretty',\n",
       " 'yay',\n",
       " 'maybe',\n",
       " 'read',\n",
       " 'live',\n",
       " 'buy',\n",
       " 'away',\n",
       " 'finally',\n",
       " 'finish',\n",
       " 'game',\n",
       " 'baby',\n",
       " 'check',\n",
       " 'damn',\n",
       " 'win',\n",
       " 'omg',\n",
       " 'summer',\n",
       " 'lt3',\n",
       " 'someone',\n",
       " 'listen',\n",
       " 'guess',\n",
       " 'break',\n",
       " 'birthday',\n",
       " 'havent',\n",
       " 'glad',\n",
       " 'nothing',\n",
       " 'wow',\n",
       " 'end',\n",
       " 'hot',\n",
       " 'bit',\n",
       " 'pic',\n",
       " 'hard',\n",
       " 'party',\n",
       " 'stuff',\n",
       " 'run',\n",
       " 'weather',\n",
       " 'bore',\n",
       " 'happen',\n",
       " 'stop',\n",
       " 'might',\n",
       " 'late',\n",
       " 'mom',\n",
       " 'later',\n",
       " 'put',\n",
       " 'also',\n",
       " 'sun',\n",
       " 'two',\n",
       " 'saw',\n",
       " 'actually',\n",
       " 'exam',\n",
       " 'yesterday',\n",
       " 'id',\n",
       " 'friday',\n",
       " 'isnt',\n",
       " 'meet',\n",
       " 'monday',\n",
       " 'sunday',\n",
       " 'ya',\n",
       " 'world',\n",
       " 'god',\n",
       " 'hi',\n",
       " 'car',\n",
       " 'tire',\n",
       " 'ugh',\n",
       " 'many',\n",
       " 'stay',\n",
       " 'seem',\n",
       " 'kid',\n",
       " 'gotta',\n",
       " 'job',\n",
       " 'send',\n",
       " 'music',\n",
       " 'update',\n",
       " 'around',\n",
       " 'since',\n",
       " 'post',\n",
       " 'book',\n",
       " 'cold',\n",
       " 'family',\n",
       " 'follower',\n",
       " 'move',\n",
       " 'sit',\n",
       " 'forget',\n",
       " 'cry',\n",
       " 'die',\n",
       " 'must',\n",
       " 'food',\n",
       " 'okay',\n",
       " 'beautiful',\n",
       " 'fuck',\n",
       " 'luck',\n",
       " 'write',\n",
       " 'sweet',\n",
       " 'anything',\n",
       " 'far',\n",
       " 'poor',\n",
       " 'drive',\n",
       " 'hair',\n",
       " 'video',\n",
       " 'till',\n",
       " 'least',\n",
       " 'almost',\n",
       " 'whats',\n",
       " 'aww',\n",
       " 'tho',\n",
       " 'dinner',\n",
       " 'lunch',\n",
       " 'funny',\n",
       " 'picture',\n",
       " 'place',\n",
       " 'mine',\n",
       " 'free',\n",
       " 'anyone',\n",
       " 'cause',\n",
       " 'idea',\n",
       " 'may',\n",
       " 'boy',\n",
       " 'drink',\n",
       " 'welcome',\n",
       " 'cute',\n",
       " 'everything',\n",
       " 'change',\n",
       " 'coffee',\n",
       " 'study',\n",
       " 'believe',\n",
       " 'month',\n",
       " 'excite',\n",
       " 'plan',\n",
       " 'couldnt',\n",
       " 'class',\n",
       " 'without',\n",
       " 'walk',\n",
       " 'shes',\n",
       " 'every',\n",
       " 'forward',\n",
       " 'room',\n",
       " 'shit',\n",
       " 'dad',\n",
       " 'turn',\n",
       " 'iphone',\n",
       " 'xx',\n",
       " 'name',\n",
       " 'probably',\n",
       " 'ask',\n",
       " 'catch',\n",
       " 'stupid',\n",
       " 'lovely',\n",
       " 'hang',\n",
       " 'totally',\n",
       " 'dog',\n",
       " 'hahaha',\n",
       " 'hopefully',\n",
       " 'sooo',\n",
       " 'add',\n",
       " 'enough',\n",
       " 'real',\n",
       " 'busy',\n",
       " 'outside',\n",
       " 'dream',\n",
       " 'money',\n",
       " 'anymore',\n",
       " 'pay',\n",
       " 'close',\n",
       " 'brother',\n",
       " 'ha',\n",
       " 'minute',\n",
       " 'tv',\n",
       " 'word',\n",
       " 'bring',\n",
       " 'whole',\n",
       " 'wasnt',\n",
       " 'tired',\n",
       " 'hit',\n",
       " 'headache',\n",
       " 'amaze',\n",
       " 'amazing',\n",
       " 'saturday',\n",
       " 'hell',\n",
       " 'mother',\n",
       " 'kill',\n",
       " 'able',\n",
       " 'computer',\n",
       " 'fan',\n",
       " 'kinda',\n",
       " 'remember',\n",
       " 'rest',\n",
       " 'rock',\n",
       " '10',\n",
       " 'problem',\n",
       " 'wrong',\n",
       " 'spend',\n",
       " 'face',\n",
       " 'clean',\n",
       " 'part',\n",
       " 'super',\n",
       " 'wonder',\n",
       " 'beach',\n",
       " 'crazy',\n",
       " 'news',\n",
       " 'blog',\n",
       " 'eye',\n",
       " 'fail',\n",
       " 'photo',\n",
       " 'reply',\n",
       " 'email',\n",
       " 'link',\n",
       " 'open',\n",
       " 'hello',\n",
       " 'goodnight',\n",
       " 'either',\n",
       " 'ago',\n",
       " 'boo',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'half',\n",
       " 'train',\n",
       " 'ah',\n",
       " 'course',\n",
       " 'full',\n",
       " 'care',\n",
       " 'heart',\n",
       " 'shower',\n",
       " 'stick',\n",
       " 'kind',\n",
       " 'sister',\n",
       " 'soo',\n",
       " 'hehe',\n",
       " 'learn',\n",
       " 'pain',\n",
       " 'theyre',\n",
       " 'fall',\n",
       " 'awww',\n",
       " 'cuz',\n",
       " 'internet',\n",
       " 'final',\n",
       " 'ticket',\n",
       " 'mind',\n",
       " 'hug',\n",
       " 'wear',\n",
       " 'feeling',\n",
       " 'quite',\n",
       " 'site',\n",
       " 'alone',\n",
       " 'breakfast',\n",
       " 'pick',\n",
       " 'test',\n",
       " 'favorite',\n",
       " 'season',\n",
       " 'worry',\n",
       " 'dude',\n",
       " 'til',\n",
       " 'office',\n",
       " 'hop',\n",
       " 'lucky',\n",
       " 'person',\n",
       " 'agree',\n",
       " 'fine',\n",
       " 'fix',\n",
       " 'facebook',\n",
       " 'hand',\n",
       " 'awake',\n",
       " 'else',\n",
       " 'text',\n",
       " 'june',\n",
       " 'cut',\n",
       " 'dance',\n",
       " 'ice',\n",
       " 'vote',\n",
       " 'concert',\n",
       " 'online',\n",
       " 'sell',\n",
       " 'suppose',\n",
       " 'set',\n",
       " 'btw',\n",
       " 'instead',\n",
       " 'la',\n",
       " 'seriously',\n",
       " 'visit',\n",
       " 'short',\n",
       " 'sign',\n",
       " 'xd',\n",
       " 'lmao',\n",
       " 'sunny',\n",
       " '100',\n",
       " 'afternoon',\n",
       " 'aw',\n",
       " 'dear',\n",
       " 'hungry',\n",
       " 'smile',\n",
       " 'asleep',\n",
       " 'couple',\n",
       " 'sore',\n",
       " 'story',\n",
       " 'album',\n",
       " 'shop',\n",
       " 'crap',\n",
       " 'jealous',\n",
       " 'together',\n",
       " 'wonderful',\n",
       " 'page',\n",
       " 'message',\n",
       " 'cat',\n",
       " 'high',\n",
       " 'join',\n",
       " 'account',\n",
       " 'bday',\n",
       " '12',\n",
       " 'date',\n",
       " 'dead',\n",
       " 'goin',\n",
       " 'lady',\n",
       " 'top',\n",
       " 'anyway',\n",
       " 'pass',\n",
       " 'youll',\n",
       " 'fly',\n",
       " 'nap',\n",
       " 'homework',\n",
       " 'reason',\n",
       " 'sigh',\n",
       " 'theres',\n",
       " 'bye',\n",
       " 'easy',\n",
       " 'ppl',\n",
       " 'store',\n",
       " 'wouldnt',\n",
       " 'pack',\n",
       " 'second',\n",
       " 'soooo',\n",
       " 'award',\n",
       " 'drop',\n",
       " '30',\n",
       " 'figure',\n",
       " 'sometimes',\n",
       " 'star',\n",
       " 'download',\n",
       " 'list',\n",
       " 'city',\n",
       " 'ipod',\n",
       " 'moment',\n",
       " 'bout',\n",
       " 'definitely',\n",
       " 'save',\n",
       " 'town',\n",
       " 'thought',\n",
       " 'decide',\n",
       " 'share',\n",
       " 'congrats',\n",
       " 'water',\n",
       " 'red',\n",
       " 'excited',\n",
       " 'laugh',\n",
       " 'yea',\n",
       " 'park',\n",
       " 'cream',\n",
       " 'holiday',\n",
       " 'kick',\n",
       " 'ride',\n",
       " 'chat',\n",
       " 'dress',\n",
       " 'lil',\n",
       " 'perfect',\n",
       " 'answer',\n",
       " 'laptop',\n",
       " 'point',\n",
       " 'realize',\n",
       " 'shopping',\n",
       " 'side',\n",
       " 'order',\n",
       " 'youtube',\n",
       " 'min',\n",
       " 'sing',\n",
       " 'line',\n",
       " 'mum',\n",
       " 'nite',\n",
       " 'math',\n",
       " 'weird',\n",
       " 'aint',\n",
       " 'less',\n",
       " 'past',\n",
       " 'unfortunately',\n",
       " 'arent',\n",
       " 'cook',\n",
       " 'evening',\n",
       " 'foot',\n",
       " 'mood',\n",
       " 'understand',\n",
       " 'mad',\n",
       " 'comment',\n",
       " 'parent',\n",
       " 'air',\n",
       " 'fast',\n",
       " 'team',\n",
       " 'tour',\n",
       " 'flight',\n",
       " 'hat',\n",
       " 'tea',\n",
       " 'via',\n",
       " 'jonas',\n",
       " 'throat',\n",
       " '1st',\n",
       " 'chocolate',\n",
       " 'episode',\n",
       " '20',\n",
       " 'gt',\n",
       " 'light',\n",
       " 'meeting',\n",
       " 'upload',\n",
       " 'ahh',\n",
       " 'slow',\n",
       " 'burn',\n",
       " 'google',\n",
       " 'gym',\n",
       " 'lay',\n",
       " 'church',\n",
       " 'finger',\n",
       " 'sleepy',\n",
       " 'worth',\n",
       " 'yall',\n",
       " 'babe',\n",
       " 'hows',\n",
       " 'mac',\n",
       " 'moon',\n",
       " 'load',\n",
       " 'upset',\n",
       " 'boring',\n",
       " 'da',\n",
       " 'fell',\n",
       " 'pool',\n",
       " 'support',\n",
       " 'three',\n",
       " 'beat',\n",
       " 'english',\n",
       " 'hmm',\n",
       " 'project',\n",
       " 'xxx',\n",
       " 'band',\n",
       " 'thx',\n",
       " 'vacation',\n",
       " 'black',\n",
       " 'chance',\n",
       " 'forever',\n",
       " 'green',\n",
       " 'scar',\n",
       " 'sunshine',\n",
       " 'voice',\n",
       " 'college',\n",
       " 'london',\n",
       " 'bus',\n",
       " 'leg',\n",
       " 'rather',\n",
       " 'throw',\n",
       " 'age',\n",
       " 'beer',\n",
       " 'card',\n",
       " 'small',\n",
       " 'website',\n",
       " 'film',\n",
       " 'horrible',\n",
       " 'mr',\n",
       " 'bet',\n",
       " 'annoy',\n",
       " 'em',\n",
       " 'sims',\n",
       " 'huge',\n",
       " 'sort',\n",
       " 'warm',\n",
       " 'cake',\n",
       " 'cd',\n",
       " 'officially',\n",
       " 'shoe',\n",
       " 'special',\n",
       " 'blue',\n",
       " 'cannot',\n",
       " 'cheer',\n",
       " 'camera',\n",
       " 'especially',\n",
       " 'paper',\n",
       " 'stand',\n",
       " 'due',\n",
       " 'garden',\n",
       " 'hold',\n",
       " 'record',\n",
       " 'flu',\n",
       " 'son',\n",
       " 'window',\n",
       " 'yep',\n",
       " 'apparently',\n",
       " 'body',\n",
       " 'lie',\n",
       " 'number',\n",
       " 'relax',\n",
       " 'ahhh',\n",
       " 'bike',\n",
       " 'smell',\n",
       " 'camp',\n",
       " 'mess',\n",
       " 'mtv',\n",
       " 'shame',\n",
       " 'club',\n",
       " 'hill',\n",
       " 'wed',\n",
       " 'bored',\n",
       " 'cancel',\n",
       " 'felt',\n",
       " 'question',\n",
       " 'rip',\n",
       " 'stomach',\n",
       " 'yummy',\n",
       " 'case',\n",
       " 'box',\n",
       " 'father',\n",
       " 'itll',\n",
       " 'shall',\n",
       " 'youve',\n",
       " 'white',\n",
       " 'fair',\n",
       " 'notice',\n",
       " 'radio',\n",
       " 'shoot',\n",
       " 'wit',\n",
       " '#followfriday',\n",
       " 'bitch',\n",
       " 'graduation',\n",
       " 'proud',\n",
       " 'scary',\n",
       " 'count',\n",
       " 'fantastic',\n",
       " 'front',\n",
       " 'thursday',\n",
       " 'tuesday',\n",
       " 'wtf',\n",
       " 'doctor',\n",
       " 'dvd',\n",
       " 'plus',\n",
       " 'search',\n",
       " 'storm',\n",
       " 'uk',\n",
       " 'yr',\n",
       " 'bless',\n",
       " 'interview',\n",
       " 'road',\n",
       " 'freak',\n",
       " 'goodbye',\n",
       " 'lazy',\n",
       " 'pull',\n",
       " 'become',\n",
       " 'idk',\n",
       " 'nope',\n",
       " 'type',\n",
       " 'crash',\n",
       " 'airport',\n",
       " 'apple',\n",
       " 'grow',\n",
       " 'lonely',\n",
       " 'pizza',\n",
       " 'si',\n",
       " 'touch',\n",
       " 'chicken',\n",
       " 'deal',\n",
       " 'fight',\n",
       " 'tom',\n",
       " 'cover',\n",
       " 'kiss',\n",
       " 'miley',\n",
       " 'myspace',\n",
       " 'note',\n",
       " 'power',\n",
       " 'safe',\n",
       " 'shot',\n",
       " 'woman',\n",
       " 'cousin',\n",
       " 'david',\n",
       " '2day',\n",
       " 'appreciate',\n",
       " 'block',\n",
       " 'boyfriend',\n",
       " 'bum',\n",
       " 'inside',\n",
       " 'issue',\n",
       " 'mile',\n",
       " 'arrive',\n",
       " 'service',\n",
       " 'wedding',\n",
       " '15',\n",
       " 'bro',\n",
       " 'different',\n",
       " 'dm',\n",
       " 'heard',\n",
       " 'round',\n",
       " 'alot',\n",
       " 'company',\n",
       " 'except',\n",
       " 'near',\n",
       " 'pop',\n",
       " 'sadly',\n",
       " 'wash',\n",
       " 'young',\n",
       " 'yup',\n",
       " 'absolutely',\n",
       " 'expect',\n",
       " 'fact',\n",
       " 'plane',\n",
       " 'along',\n",
       " 'chill',\n",
       " 'stress',\n",
       " 'alright',\n",
       " 'hes',\n",
       " 'speak',\n",
       " 'state',\n",
       " 'whatever',\n",
       " 'bbq',\n",
       " 'glass',\n",
       " 'invite',\n",
       " 'july',\n",
       " 'tear',\n",
       " 'deserve',\n",
       " 'revision',\n",
       " 'although',\n",
       " 'ball',\n",
       " 'gorgeous',\n",
       " 'itunes',\n",
       " 'woo',\n",
       " 'arm',\n",
       " 'joke',\n",
       " 'luv',\n",
       " 'piss',\n",
       " 'pray',\n",
       " 'quot',\n",
       " 'release',\n",
       " 'tummy',\n",
       " 'twilight',\n",
       " '#fb',\n",
       " 'hubby',\n",
       " 'ouch',\n",
       " 'bb',\n",
       " 'business',\n",
       " 'cup',\n",
       " 'dark',\n",
       " 'matter',\n",
       " 'promise',\n",
       " 'track',\n",
       " 'wednesday',\n",
       " 'ache',\n",
       " 'hmmm',\n",
       " 'longer',\n",
       " 'nick',\n",
       " 'pc',\n",
       " 'roll',\n",
       " '2nd',\n",
       " 'everybody',\n",
       " 'shirt',\n",
       " 'taste',\n",
       " 'version',\n",
       " 'act',\n",
       " 'history',\n",
       " 'exactly',\n",
       " 'fire',\n",
       " 'french',\n",
       " 'profile',\n",
       " 'app',\n",
       " 'clothes',\n",
       " 'lame',\n",
       " 'rainy',\n",
       " 'single',\n",
       " 'street',\n",
       " 'tan',\n",
       " 'waste',\n",
       " 'wife',\n",
       " 'bloody',\n",
       " 'door',\n",
       " 'hr',\n",
       " 'ruin',\n",
       " '11',\n",
       " 'begin',\n",
       " 'fit',\n",
       " 'hahah',\n",
       " 'hospital',\n",
       " 'lately',\n",
       " 'nobody',\n",
       " 'bug',\n",
       " 'dang',\n",
       " 'daughter',\n",
       " 'group',\n",
       " 'low',\n",
       " 'mommy',\n",
       " 'puppy',\n",
       " 'vip',\n",
       " 'allow',\n",
       " 'bc',\n",
       " 'country',\n",
       " 'mall',\n",
       " 'silly',\n",
       " 'traffic',\n",
       " 'wine',\n",
       " 'behind',\n",
       " 'currently',\n",
       " 'def',\n",
       " 'gettin',\n",
       " 'often',\n",
       " 'paint',\n",
       " 'usually',\n",
       " 'art',\n",
       " 'bag',\n",
       " 'bear',\n",
       " 'ear',\n",
       " 'fat',\n",
       " 'ohh',\n",
       " 'completely',\n",
       " 'graduate',\n",
       " 'hangover',\n",
       " 'interesting',\n",
       " 'john',\n",
       " 'jus',\n",
       " 'mate',\n",
       " 'random',\n",
       " 'taylor',\n",
       " 'travel',\n",
       " 'yo',\n",
       " 'congratulation',\n",
       " 'drunk',\n",
       " 'eh',\n",
       " 'major',\n",
       " 'others',\n",
       " 'terrible',\n",
       " 'twit',\n",
       " 'nail',\n",
       " 'raining',\n",
       " 'return',\n",
       " 'sat',\n",
       " 'watchin',\n",
       " 'xoxo',\n",
       " 'feed',\n",
       " 'gay',\n",
       " 'hotel',\n",
       " 'lakers',\n",
       " 'revise',\n",
       " 'sale',\n",
       " 'serious',\n",
       " 'surprise',\n",
       " 'swim',\n",
       " 'cheese',\n",
       " 'interest',\n",
       " 'nose',\n",
       " 'race',\n",
       " 'american',\n",
       " 'de',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'mention',\n",
       " 'sky',\n",
       " 'web',\n",
       " 'lesson',\n",
       " 'somewhere',\n",
       " 'bite',\n",
       " 'blow',\n",
       " 'design',\n",
       " 'fml',\n",
       " 'hasnt',\n",
       " 'kitty',\n",
       " 'practice',\n",
       " 'quotthe',\n",
       " 'remind',\n",
       " 'child',\n",
       " 'na',\n",
       " 'ny',\n",
       " 'bank',\n",
       " 'blah',\n",
       " 'board',\n",
       " 'event',\n",
       " 'info',\n",
       " 'dunno',\n",
       " 'ooh',\n",
       " 'review',\n",
       " 'rid',\n",
       " 'bar',\n",
       " 'epic',\n",
       " 'fever',\n",
       " 'fill',\n",
       " 'kate',\n",
       " 'memory',\n",
       " 'nearly',\n",
       " 'blood',\n",
       " 'complete',\n",
       " 'cross',\n",
       " 'screen',\n",
       " 'shut',\n",
       " 'wet',\n",
       " 'buddy',\n",
       " 'celebrate',\n",
       " 'color',\n",
       " 'death',\n",
       " 'gosh',\n",
       " 'indeed',\n",
       " 'peace',\n",
       " 'pink',\n",
       " 'quiet',\n",
       " 'sexy',\n",
       " 'thinking',\n",
       " 'australia',\n",
       " 'brain',\n",
       " 'channel',\n",
       " 'choice',\n",
       " 'co',\n",
       " 'nah',\n",
       " 'peep',\n",
       " 'step',\n",
       " 'yum',\n",
       " 'none',\n",
       " 'normal',\n",
       " 'quick',\n",
       " 'series',\n",
       " 'spot',\n",
       " 'trouble',\n",
       " 'background',\n",
       " 'bird',\n",
       " 'bother',\n",
       " 'chris',\n",
       " 'huh',\n",
       " 'mouth',\n",
       " 'self',\n",
       " 'shift',\n",
       " 'shouldnt',\n",
       " 'ahead',\n",
       " 'argh',\n",
       " 'possible',\n",
       " 'awful',\n",
       " 'earlier',\n",
       " 'future',\n",
       " 'land',\n",
       " 'sense',\n",
       " 'steal',\n",
       " '25',\n",
       " 'awwww',\n",
       " 'bunch',\n",
       " 'copy',\n",
       " 'daddy',\n",
       " 'fb',\n",
       " 'jon',\n",
       " 'quoti',\n",
       " 'session',\n",
       " 'teeth',\n",
       " 'chillin',\n",
       " 'doubt',\n",
       " 'guitar',\n",
       " 'key',\n",
       " 'manage',\n",
       " 'quote',\n",
       " 'starbucks',\n",
       " 'stuck',\n",
       " 'tip',\n",
       " 'trek',\n",
       " 'afford',\n",
       " 'blackberry',\n",
       " 'husband',\n",
       " ...]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "# Make vocab for text and label fields\n",
    "txt_field.build_vocab(dataset,\n",
    "                      max_size = MAX_VOCAB_SIZE, \n",
    "                      vectors = vocab.Vectors('glove.twitter.27B.50d.txt', '../model/glove.twitter.27B/'))\n",
    "label_field.build_vocab(dataset)\n",
    "\n",
    "pretrained_embeddings = txt_field.vocab.vectors\n",
    "\n",
    "# Make train/val/test splits\n",
    "train_data, test_data, valid_data = dataset.split([0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 70000\n",
      "Number of validation examples: 20000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                          batch_sizes = (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "                                                                        sort_key = lambda x: len(x.SentimentText),\n",
    "                                                                        sort_within_batch=True,\n",
    "                                                                        repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):#, optimizer, criterion):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           #bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.optimizer = None #optimizer #optim.Adam(model.parameters())\n",
    "        self.criterion = None #criterion #nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    \n",
    "    def set_pretrained_weights(self, pretrained_embeddings, txt_field):\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "        UNK_IDX = txt_field.vocab.stoi[txt_field.unk_token]\n",
    "\n",
    "        self.embedding.weight.data[UNK_IDX] = torch.zeros(self.embedding_dim)\n",
    "        self.embedding.weight.data[self.pad_idx] = torch.zeros(self.embedding_dim)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.train()\n",
    "        optimizer = self.optimizer\n",
    "        for batch in iterator:\n",
    "            if len(batch) == BATCH_SIZE:\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text, text_lengths = batch.SentimentText\n",
    "                predictions = self.forward(text, text_lengths).squeeze(1)\n",
    "                loss = self.criterion(predictions, batch.Sentiment.float())\n",
    "\n",
    "                acc = self.binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "\n",
    "                    text, text_lengths = batch.SentimentText\n",
    "\n",
    "                    predictions = self.forward(text, text_lengths).squeeze(1)\n",
    "\n",
    "                    loss = self.criterion(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    acc = self.binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def sentiment_political(self, iterator):\n",
    "        # keep?\n",
    "        #model.eval()\n",
    "        \n",
    "        sentiment_df = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_no = 0\n",
    "\n",
    "            for batch in iterator:\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "\n",
    "                    text, text_lengths = batch.SentimentText\n",
    "\n",
    "                    predictions = self.forward(text, text_lengths).squeeze(1)\n",
    "                    \n",
    "                    preds = torch.sigmoid(predictions)\n",
    "                    \n",
    "                    pred_arr = preds.numpy()\n",
    "                    id_arr = batch.Id.numpy()\n",
    "                    pred_class_arr = torch.round(preds).numpy()\n",
    "                    \n",
    "                    if batch_no == 0:\n",
    "                        sentiment_df = pd.DataFrame({\"tweet_id\": id_arr, \n",
    "                                                     \"prediction_raw\": pred_arr, \n",
    "                                                     \"prediction_class\": pred_class_arr})\n",
    "                    else:\n",
    "                        sentiment_df = pd.concat([sentiment_df, pd.DataFrame({\"tweet_id\": id_arr, \n",
    "                                                     \"prediction_raw\": pred_arr, \n",
    "                                                     \"prediction_class\": pred_class_arr})])\n",
    "                    batch_no += 1\n",
    "            \n",
    "\n",
    "                    #loss = self.criterion(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    #acc = self.binary_accuracy(predictions, batch.Sentiment.float())\n",
    "\n",
    "                    #epoch_loss += loss.item()\n",
    "                    #epoch_acc += acc.item()\n",
    "        return sentiment_df\n",
    "    \n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_training(N_EPOCHS, model, train_iterator, valid_iterator):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = model.train_epoch(train_iterator)\n",
    "        valid_loss, valid_acc = model.evaluate(valid_iterator)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = model.epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\"Adam\"]\n",
    "criterions = [\"BCEWithLogitsLoss\"] \n",
    "learning_rates = [0.001]\n",
    "epochs = [5] \n",
    "weight_decay = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(txt_field.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = txt_field.vocab.stoi[txt_field.pad_token]\n",
    "\n",
    "\n",
    "best_model_tuple = None\n",
    "best_accuracy = -1\n",
    "for optimizer in optimizers:\n",
    "    # Make a new model at the start\n",
    "    model = LSTM(INPUT_DIM, \n",
    "                EMBEDDING_DIM, \n",
    "                HIDDEN_DIM, \n",
    "                OUTPUT_DIM, \n",
    "                N_LAYERS,\n",
    "                DROPOUT, \n",
    "                PAD_IDX)\n",
    "    model.set_pretrained_weights(pretrained_embeddings, txt_field)\n",
    "    for learning_rate in learning_rates:\n",
    "        for decay in weight_decay:\n",
    "            # Set OPTIMIZER\n",
    "            if optimizer == \"Adam\":\n",
    "                model.optimizer = optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay = decay) \n",
    "            else: \n",
    "                model.optimizer = optim.Adagrad(params = model.parameters(), lr = learning_rate, weight_decay = decay)\n",
    "\n",
    "            for criterion in criterions:\n",
    "                # SET CRITERION\n",
    "                if criterion == \"BCEWithLogitsLoss\":\n",
    "                    model.criterion = nn.BCEWithLogitsLoss() \n",
    "                else: \n",
    "                    model.criterion = nn.NLLLoss()\n",
    "                \n",
    "                for epoch in epochs:\n",
    "                    model_name = \"-\".join([optimizer, str(learning_rate), str(decay), str(criterion), str(epoch)])\n",
    "\n",
    "                    print(f\"working on model {model_name}\")\n",
    "                    trained_model = epoch_training(epoch, model, train_iterator, valid_iterator)\n",
    "                    _, acc = trained_model.evaluate(valid_iterator)\n",
    "                    \n",
    "                    if acc > best_accuracy:\n",
    "                        best_accuracy = acc\n",
    "                        best_model_tuple = (model_name, trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = best_model_tuple[0]\n",
    "best_model = best_model_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam-0.001-0-BCEWithLogitsLoss-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4839125668546956, 0.7624402866242038)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_model_name)\n",
    "best_model.evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alenastern/.virtualenvs/aml/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "run_no = \"final\"\n",
    "torch.save(best_model.state_dict(), 'models_store/best_model_dict_{}_{}.pt'.format(best_model_name, run_no))\n",
    "torch.save(best_model, 'models_store/best_model_{}_{}.pt'.format(best_model_name, run_no))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select tweet_text_clean, tweet_id, tweet_text_raw, leadership, democrat, health, econ_jobs, immigration, taxes from staging.master WHERE health::int = 1 OR econ_jobs::int = 1 OR immigration::int = 1 OR taxes::int = 1 \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "num_tweets = 100000\n",
    "topics = ['health', 'econ_jobs', 'immigration', 'taxes']\n",
    "topics_str = ', '.join(topics)\n",
    "query_start = \"select tweet_text_clean, tweet_id, tweet_text_raw, leadership, democrat, {} from staging.master WHERE \".format(topics_str)\n",
    "for topic in topics: \n",
    "    query_start += \"{}::int = 1 OR \".format(topic)\n",
    "query_start = query_start[:-3]\n",
    "\n",
    "\n",
    "query = query_start\n",
    "print(query)\n",
    "db.conn.rollback()\n",
    "examples_db = db.read(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_train = [(example[0], example[1]) for example in examples_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['west', 'coast', 'attempt', 'literally', 'limit', 'first', 'amendment', 'punish', 'business', 'create', 'job', 'idolized', 'worship', 'pornstar', 'reason', 'try', 'make', 'trump', 'look', 'bad', 'thank', 'god', 'im', 'safe', 'side', 'country']\",\n",
       " '1000005833374265344')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fields\n",
    "txt_field_eval = data.Field(sequential=True, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True)\n",
    "\n",
    "txt_field_eval.vocab = txt_field.vocab\n",
    "id_field_eval = data.Field(sequential=False, \n",
    "                      use_vocab=False, \n",
    "                      pad_token=None, \n",
    "                      unk_token=None)\n",
    "\n",
    "eval_val_fields = [\n",
    "    ('SentimentText', txt_field_eval), # process it as text\n",
    "    ('Id', id_field_eval) # process it as id\n",
    "]\n",
    "\n",
    "\n",
    "# Convert text examples to Example datatype\n",
    "examples = [data.Example.fromlist(((ast.literal_eval(example[0])), example[1]), eval_val_fields) for example in examples_train]\n",
    "\n",
    "# Create dataset\n",
    "dataset = data.Dataset(examples, eval_val_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1259,  0.7006,  0.1237,  ...,  0.2219, -0.7194,  0.7646],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2846,  1.2260, -0.9739,  ..., -0.1293, -0.4296, -0.3433]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_field_eval.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = examples[0]\n",
    "test_example.SentimentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iterator\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "eval_iterator = data.BucketIterator(dataset, \n",
    "                                           batch_size = BATCH_SIZE, \n",
    "                                           sort_key = lambda x: len(x.SentimentText),\n",
    "                                           sort_within_batch=True,\n",
    "                                           repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(25002, 50, padding_idx=1)\n",
       "  (rnn): LSTM(50, 256, num_layers=2, dropout=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "final_model = torch.load('models_store/best_model_Adam-0.001-0-BCEWithLogitsLoss-5_1.pt')\n",
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment of each tweet\n",
    "sentiment_df = best_model.sentiment_political(eval_iterator)\n",
    "sentiment_df.to_csv('models_store/political_sentiment_{}_{}.csv'.format(best_model_name, run_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction_class\n",
       "0.0    37702\n",
       "1.0    26746\n",
       "dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.groupby('prediction_class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>prediction_raw</th>\n",
       "      <th>prediction_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>887831325113278466</td>\n",
       "      <td>0.666367</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1027215357826584576</td>\n",
       "      <td>0.355783</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>677572636344516608</td>\n",
       "      <td>0.526792</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32586949248155648</td>\n",
       "      <td>0.518794</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1058366556051128320</td>\n",
       "      <td>0.176461</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id  prediction_raw  prediction_class\n",
       "0   887831325113278466        0.666367               1.0\n",
       "1  1027215357826584576        0.355783               0.0\n",
       "2   677572636344516608        0.526792               1.0\n",
       "3    32586949248155648        0.518794               1.0\n",
       "4  1058366556051128320        0.176461               0.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df['tweet_id'] = pd.to_numeric(sentiment_df['tweet_id'])\n",
    "sentiment_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.DataFrame(examples_db, columns = ['tweet_text_clean', 'tweet_id', \n",
    "                                            'tweet_text_raw', 'leadership', 'democrat'] + topics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text_clean</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text_raw</th>\n",
       "      <th>leadership</th>\n",
       "      <th>democrat</th>\n",
       "      <th>health</th>\n",
       "      <th>econ_jobs</th>\n",
       "      <th>immigration</th>\n",
       "      <th>taxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['west', 'coast', 'attempt', 'literally', 'lim...</td>\n",
       "      <td>1000005833374265344</td>\n",
       "      <td>So the west coast has now attempted to literal...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['democrat', 'foreign', 'invader', 'godgiven',...</td>\n",
       "      <td>1000007055065911296</td>\n",
       "      <td>Democrats are foreign invaders on our God-give...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['help', 'im', 'try', 'find', 'plan', 'make', ...</td>\n",
       "      <td>1000007673193140225</td>\n",
       "      <td>HELP! I'm trying to find @RepTrey's plan for m...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['bring', 'cost', 'health', 'care', 'cover', '...</td>\n",
       "      <td>1000007675684507650</td>\n",
       "      <td>We CAN bring down the costs of health care, an...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['good', 'job', 'gop', 'harm', 'worker']</td>\n",
       "      <td>1000008098684252161</td>\n",
       "      <td>Good job by @SpeakerRyan @SenateMajLdr &amp;amp; t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    tweet_text_clean             tweet_id  \\\n",
       "0  ['west', 'coast', 'attempt', 'literally', 'lim...  1000005833374265344   \n",
       "1  ['democrat', 'foreign', 'invader', 'godgiven',...  1000007055065911296   \n",
       "2  ['help', 'im', 'try', 'find', 'plan', 'make', ...  1000007673193140225   \n",
       "3  ['bring', 'cost', 'health', 'care', 'cover', '...  1000007675684507650   \n",
       "4           ['good', 'job', 'gop', 'harm', 'worker']  1000008098684252161   \n",
       "\n",
       "                                      tweet_text_raw  leadership democrat  \\\n",
       "0  So the west coast has now attempted to literal...       False     True   \n",
       "1  Democrats are foreign invaders on our God-give...       False     True   \n",
       "2  HELP! I'm trying to find @RepTrey's plan for m...        True     True   \n",
       "3  We CAN bring down the costs of health care, an...        True     True   \n",
       "4  Good job by @SpeakerRyan @SenateMajLdr &amp; t...       False    False   \n",
       "\n",
       "   health  econ_jobs  immigration  taxes  \n",
       "0   False       True        False  False  \n",
       "1   False       True        False  False  \n",
       "2    True      False        False  False  \n",
       "3    True      False        False  False  \n",
       "4   False       True        False  False  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full['tweet_id'] = pd.to_numeric(data_full['tweet_id'])\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text_clean</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text_raw</th>\n",
       "      <th>leadership</th>\n",
       "      <th>democrat</th>\n",
       "      <th>health</th>\n",
       "      <th>econ_jobs</th>\n",
       "      <th>immigration</th>\n",
       "      <th>taxes</th>\n",
       "      <th>prediction_raw</th>\n",
       "      <th>prediction_class</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['west', 'coast', 'attempt', 'literally', 'lim...</td>\n",
       "      <td>1000005833374265344</td>\n",
       "      <td>So the west coast has now attempted to literal...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.418391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['democrat', 'foreign', 'invader', 'godgiven',...</td>\n",
       "      <td>1000007055065911296</td>\n",
       "      <td>Democrats are foreign invaders on our God-give...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.171387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['help', 'im', 'try', 'find', 'plan', 'make', ...</td>\n",
       "      <td>1000007673193140225</td>\n",
       "      <td>HELP! I'm trying to find @RepTrey's plan for m...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.600075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['bring', 'cost', 'health', 'care', 'cover', '...</td>\n",
       "      <td>1000007675684507650</td>\n",
       "      <td>We CAN bring down the costs of health care, an...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.533935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['good', 'job', 'gop', 'harm', 'worker']</td>\n",
       "      <td>1000008098684252161</td>\n",
       "      <td>Good job by @SpeakerRyan @SenateMajLdr &amp;amp; t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.495701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    tweet_text_clean             tweet_id  \\\n",
       "0  ['west', 'coast', 'attempt', 'literally', 'lim...  1000005833374265344   \n",
       "1  ['democrat', 'foreign', 'invader', 'godgiven',...  1000007055065911296   \n",
       "2  ['help', 'im', 'try', 'find', 'plan', 'make', ...  1000007673193140225   \n",
       "3  ['bring', 'cost', 'health', 'care', 'cover', '...  1000007675684507650   \n",
       "4           ['good', 'job', 'gop', 'harm', 'worker']  1000008098684252161   \n",
       "\n",
       "                                      tweet_text_raw  leadership democrat  \\\n",
       "0  So the west coast has now attempted to literal...       False     True   \n",
       "1  Democrats are foreign invaders on our God-give...       False     True   \n",
       "2  HELP! I'm trying to find @RepTrey's plan for m...        True     True   \n",
       "3  We CAN bring down the costs of health care, an...        True     True   \n",
       "4  Good job by @SpeakerRyan @SenateMajLdr &amp; t...       False    False   \n",
       "\n",
       "   health  econ_jobs  immigration  taxes  prediction_raw  prediction_class  \\\n",
       "0   False       True        False  False        0.418391               0.0   \n",
       "1   False       True        False  False        0.171387               0.0   \n",
       "2    True      False        False  False        0.600075               1.0   \n",
       "3    True      False        False  False        0.533935               1.0   \n",
       "4   False       True        False  False        0.495701               0.0   \n",
       "\n",
       "   negative  positive  \n",
       "0         1         0  \n",
       "1         1         0  \n",
       "2         0         1  \n",
       "3         0         1  \n",
       "4         1         0  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_data = data_full.merge(sentiment_df, on = 'tweet_id')\n",
    "pol_data['negative'] = np.where((pol_data['prediction_class'] == 0), 1, 0)\n",
    "pol_data['positive'] = np.where((pol_data['prediction_class'] == 1), 1, 0)\n",
    "pol_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_data.to_csv('pol_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = ['dem-lead', 'dem-base', 'rep-lead', 'rep-base']\n",
    "palette = {\"dem-base\": \"#a8b2ff\",\"dem-lead\": \"#0015bc\", \"rep-base\": \"#ff9d9d\", \"rep-lead\": \"#ff0000\", \"\": \"gray\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_for_plotting(df):\n",
    "    df['group'] = np.where((df['democrat']==True) & (df['leadership']==True), 'dem-lead', \n",
    "        np.where((df['democrat']==True) & (df['leadership']==False), 'dem-base',\n",
    "        np.where((df['democrat']==False) & (df['leadership']==True), 'rep-lead', \n",
    "        np.where((df['democrat']==False) & (df['leadership']==False), 'rep-base', \"\"))))\n",
    "\n",
    "    plotting = pd.melt(df[df['group'] != \"\"], id_vars = ['democrat', 'leadership', 'group', 'tweet_id', 'tweet_text_raw', 'tweet_text_clean', 'prediction_class', 'prediction_raw'])\n",
    "\n",
    "    return plotting\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(plotting, palette, topic, neg_pos_only = True):\n",
    "    if neg_pos_only:\n",
    "        data = plotting[plotting['variable'].isin(['negative', 'positive'])]\n",
    "    else:\n",
    "        data = plotting\n",
    "    plt.figure(figsize = (6, 3.5))\n",
    "    sns_plot = sns.barplot(x='variable', y='value', hue='group', data=data, palette=palette)\n",
    "   #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    sns_plot.figure.savefig(\"plots/{}.png\".format(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    topic_data = pol_data[pol_data[topic] == True]\n",
    "    topic_data.drop(topics, axis=1, inplace=True)\n",
    "    topic_data_plot = results_for_plotting(topic_data)\n",
    "    plot(topic_data_plot, palette, topic)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
